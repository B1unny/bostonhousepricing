/usr/bin/chmod 755 /usr/bin/turbostat
logger "EPILOG[INFO][$SLURM_JOB_ID]: Starting clean Epilog for job $SLURM_JOB_ID"
if [ x$SLURM_JOB_UID == "x" ] ; then
        exit 0
fi
if [ x$SLURM_JOB_ID == "x" ] ; then
        exit 0
fi
#SLURM_JOB_ID=$1
data=$(scontrol show job $SLURM_JOB_ID)
output_file_path=$(echo "$data" | grep -E "StdOut" | cut -d '=' -f 2-)
error_file_path=$(echo "$data" | grep -E "StdErr" | cut -d '=' -f 2-)
if [ -z "$output_file_path" ]; then
  echo "Output file does not exist"
else
common_path="/home/omjadhav/slurm-project/OUTPUT"
mkdir $common_path/$SLURM_JOB_ID
output_file_dest="$common_path/$SLURM_JOB_ID/$SLURM_JOB_ID.out"
error_file_dest="$common_path/$SLURM_JOB_ID/$SLURM_JOB_ID.err"
cp $output_file_path $output_file_dest
cp $error_file_path $error_file_dest
fi
# Don't try to kill user root or system daemon jobs
if [ $SLURM_JOB_UID -lt 1000 ] ; then
        exit 0
fi
# this should fix problem of new user jobs comming when this script is executing
user_procs=$(pgrep -u "$SLURM_JOB_UID")
user_shm_files=$(find /dev/shm -name 'psm_shm.*')
user_tmp_files=$(find /local_scratch /tmp /usr/tmp /var/tmp -uid "$SLURM_JOB_UID")
#TMP_DIRS="/dev/shm /tmp /usr/tmp /var/tmp /local_scratch"
cgroup_uid_dir="/sys/fs/cgroup/cpuset/slurm/uid_$SLURM_JOB_UID"
job_list=$([ -d $cgroup_uid_dir ] && (cd $cgroup_uid_dir; ls -d job_* | awk '/job_[0-9][0-9]*/{sub(/^job_/,""); print}'))
# Clean Job actual job cgroups and processes
# this should correct job zombies bug and clean what
# Slurm doesn't clean correctly (processes for non existant jobs in squeue and cgroups)
# we find all processes for actual job and we kill them
logger "EPILOG[INFO][$SLURM_JOB_ID]: killing processes of job $SLURM_JOB_ID"
find "$cgroup_uid_dir/job_$SLURM_JOB_ID" -name "tasks" -exec kill {} /dev/null \; 2> /dev/null
# Then we delete cgroups
#
logger "EPILOG[INFO][$SLURM_JOB_ID]: deleting cgroups of job $SLURM_JOB_ID"
CONTROLLERS="cpuset freezer"
for CONTROLLER in $CONTROLLERS; do
        CONTROLLER_JOB_DIR="/sys/fs/cgroup/$CONTROLLER/slurm/uid_$SLURM_JOB_UID/job_$SLURM_JOB_ID"
        if [ ! -d "$CONTROLLER_JOB_DIR" ]; then
                logger "EPILOG[INFO][$SLURM_JOB_ID]: cgroups files not found for $CONTROLLER_JOB_DIR"
        else
                logger "EPILOG[ERROR][$SLURM_JOB_ID]: cgroups files found for $CONTROLLER_JOB_DIR"
                number_step_dirs=$(find $CONTROLLER_JOB_DIR -name "step_*" -type d | wc -l)
                logger "EPILOG[ERROR][$SLURM_JOB_ID]: number of step_* directories is $number_step_dirs"
                # find -delete processes in-depth first
                find $CONTROLLER_JOB_DIR -type d -delete
        fi
done
for job_id in $job_list; do
        if [ "$job_id" != "$SLURM_JOB_ID" ]; then
                logger "EPILOG[INFO][$SLURM_JOB_ID]: another JOB $job_id is running for user $SLURM_UID ($SLURM_JOB_ID) on $HOSTNAME"
                exit 0
        fi
done
# No other SLURM jobs, purge all remaining processes of this user
logger "EPILOG[INFO][$SLURM_JOB_ID]: no other JOB were found for user $SLURM_JOB_UID ($SLURM_JOB_ID) on $HOSTNAME"
# Ensure user cgroup is removed if user has no other running job on this node
for CONTROLLER in $CONTROLLERS; do
        rmdir /sys/fs/cgroup/$CONTROLLER/slurm/uid_$SLURM_JOB_UID
done
for pid in $user_procs;
do
        kill -9 $pid;
        logger "EPILOG[INFO][$SLURM_JOB_ID]: killed remaining running process ($pid) of user ${SLURM_JOB_UID} for JOB ${SLURM_JOB_ID} on ${HOSTNAME}"
done
# clean shared memory files generated by QLogic PSM stack
logger "EPILOG[INFO][$SLURM_JOB_ID]: Cleaning shared memory files generated by QLogic PSM stack for job $SLURM_JOB_ID"
for shm_file in $user_shm_files
do
        rm -f $shm_file
done
# Drop clean caches
echo 3 >/proc/sys/vm/drop_caches
# clean /tmp /local_scratch
logger "EPILOG[INFO][$SLURM_JOB_ID]: Deleting tmp files for job $SLURM_JOB_ID"
IFS=$'\n'
for tmp_file in $user_tmp_files
do
        logger "EPILOG[INFO][$SLURM_JOB_ID]: tmp delete $tmp_file"
        rm -rf $tmp_file
done
# Remove any remaining temporary files the user created.
#for tmpdir in $TMP_DIRS; do
#    find "$tmpdir" -uid $SLURM_JOB_UID -exec rm -Rf {} +
#    find_retval=$?
#
#    if [[ $find_retval -gt 0 ]]; then
#        echo "Epilog error - unable to clean up temp files in $tmpdir"
#        exit $find_retval
#   fi
#done
IFS=''
# Exit cleanly when finishing
exit 0